# -*- coding: utf-8 -*-
"""
Shakespeare GPT - GPUT4

Automatically generated by Colab.

## Description
This script trains a lightweight GPT model for generating Shakespearean-style text.

## Hardware
- **GPU Used:** Tesla T4 (provided by Google Colab)
- **Memory:** 16 GB GDDR6
- **Compute Capability:** 7.5

## Software
- **Framework:** PyTorch
- **Python Version:** 3.10+
- **Environment:** Google Colab

## Notes
Ensure that GPU acceleration is enabled in the Colab runtime settings for optimal training performance.
"""


import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from IPython.display import clear_output
import time

# Hyperparameters
BATCH_SIZE = 32
BLOCK_SIZE = 64
LEARNING_RATE = 3e-4
MAX_EPOCHS = 10
EVAL_INTERVAL = 20
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME = 'Shakespeare'

torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.benchmark = True

class BinaryDataset(Dataset):
    def __init__(self, data_path):
        self.data = torch.from_numpy(np.fromfile(data_path, dtype=np.uint8).astype(np.int64))
    def __len__(self): return len(self.data) - BLOCK_SIZE
    def __getitem__(self, idx):
        chunk = self.data[idx:idx + BLOCK_SIZE + 1]
        return chunk[:-1], chunk[1:]

class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):
        super().__init__()
        self.block_size = block_size
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.blocks = nn.ModuleList([
            nn.ModuleList([
                nn.LayerNorm(n_embd),
                nn.MultiheadAttention(n_embd, n_head, dropout=dropout),
                nn.LayerNorm(n_embd),
                nn.Sequential(
                    nn.Linear(n_embd, 4 * n_embd),
                    nn.GELU(),
                    nn.Linear(4 * n_embd, n_embd),
                    nn.Dropout(dropout)
                )
            ]) for _ in range(n_layer)
        ])
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        tok_emb = self.token_embedding(idx)  # Token embeddings
        pos_emb = self.position_embedding(torch.arange(T, device=DEVICE))  # Positional embeddings
        x = tok_emb + pos_emb  # Combine token and positional embeddings
        x = x.transpose(0, 1)  # Switch to shape (T, B, n_embd) for attention layers

        for ln1, mha, ln2, feedforward in self.blocks:
            x = x + mha(ln1(x), ln1(x), ln1(x))[0]  # Self-attention
            x = x + feedforward(ln2(x))  # Feedforward

        x = x.transpose(0, 1)  # Switch back to shape (B, T, n_embd)
        x = self.ln_f(x)  # Final layer normalization
        logits = self.lm_head(x)  # Output logits

        loss = None
        if targets is not None:
            logits_flat = logits.view(-1, logits.size(-1))
            targets_flat = targets.view(-1)
            loss = F.cross_entropy(logits_flat, targets_flat)

        return logits, loss


def train_model():
    # Initialize data
    train_dataset = BinaryDataset('/content/train.bin')
    val_dataset = BinaryDataset('/content/val.bin')
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)

    # Model setup
    model = GPTLanguageModel(
        vocab_size=280,
        n_embd=128,
        block_size=BLOCK_SIZE,
        n_head=16,
        n_layer=12,
        dropout=0.1
    ).to(DEVICE)

    # Training setup
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, MAX_EPOCHS)
    scaler = torch.cuda.amp.GradScaler()

    # Training tracking
    train_losses, val_losses = [], []
    best_val_loss = float('inf')

    print(f"Training {MODEL_NAME} on {DEVICE}")
    print(f"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

    for epoch in range(MAX_EPOCHS):
        model.train()
        epoch_loss = 0
        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{MAX_EPOCHS}')

        for i, (x, y) in pbar:
            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)

            with torch.cuda.amp.autocast():
                logits, loss = model(x, y)

            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()

            epoch_loss += loss.item()
            train_losses.append(loss.item())
            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.6f}'})

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)
                with torch.cuda.amp.autocast():
                    _, loss = model(x, y)
                val_loss += loss.item()
        val_loss /= len(val_loader)
        val_losses.append(val_loss)

        # Plot progress
        clear_output(wait=True)
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses, label='Train', alpha=0.5)
        plt.plot(val_losses, label='Val')
        plt.title(f'{MODEL_NAME} Training Progress')
        plt.legend()
        plt.show()

        print(f"\nEpoch {epoch+1} Summary:")
        print(f"Train Loss: {epoch_loss/len(train_loader):.4f}")
        print(f"Val Loss: {val_loss:.4f}")
        print(f"GPU Memory: {torch.cuda.max_memory_allocated()/1e9:.2f}GB")

        scheduler.step()

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
            }, f'{MODEL_NAME}_best.pt')

if __name__ == '__main__':
    train_model()